{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "  <td><img src=\"media/sparc_logo.png\" width=200></td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pit Henrich - Surgical Planning and Robotic Cognition (SPARC) - FAU Erlangen-NÃ¼rnberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Neural Networks: Classification\n",
    "### Transforming  a Feature Space\n",
    "Classification is the process of assigning a data point to a class.\n",
    "A data point is defined by its features, the classification is therefore done in a space called the feature space.\n",
    "Neural networks can help us transform the feature space in a way such that classes are easier to separate.\n",
    "Notice that in the example below, the blue and red lines belong to two different classes and can be separated by a linear function once the feature space has been transformed by a neural network.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif\" alt=\"Topology\" style=\"width: 400px;\"/>\n",
    "    <figcaption>State space transformation example from <a href=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\">[ref]</a>\n",
    "</figure>\n",
    "    \n",
    "The more layers, and neurons in those layers (see next section), are used, the more complex transformations can be represented by a neural network.\n",
    "This is often referred to as the capacity of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neurons\n",
    "Artificial neurons are inspired by their biological counterpart. The output of neurons is based on the weighted sum of multiple inputs and a nonlinear activation function (a threshold for the biological neuron). The output of each neuron may be the input of other neurons, creating a complex wired network.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png\" alt=\"Neuron\" style=\"width: 500px;\"/>\n",
    "    <figcaption><i>Schematic of a biological and artificial neuron</i> <a href=\"https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7\">[ref]</a>\n",
    "</figure>\n",
    "    \n",
    "It is important to mention, that artificial neurons do not claim to work like biological neurons or to simulate them. They are simple inspired by them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron\n",
    "The most common type of neural networks is the so-called Multi Layer Perceptron (MLP).\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/280px-Colored_neural_network.svg.png\" alt=\"MLP\" style=\"width: 250px;\"/>\n",
    "    <figcaption><i>Schematic of a simple neural network with a single hidden layer </i><a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">[ref]</a>\n",
    "</figure>\n",
    "    \n",
    "An MLP consists of artificial neurons that are arranged in layers where information is processed from inputs\n",
    "to outputs in a feedforward fashion (no loops).\n",
    "Each neuron (circle) receives all outputs of the previous layer as inputs.\n",
    "The inputs **x** are processed as a weighted sum with an additional bias term ($\\sum_iw_ix_i + b$),\n",
    "and passed through a non-linear activation $f$ function to output a scalar value $y = f(\\sum_iw_ix_i + b)$.\n",
    "Non-linearity of the activation function is crucial,\n",
    "because otherwise all the layers in an MLP could be reduced to a single layer MLP.\n",
    "While an MLP with a single layer can [theoretically represent any function](https://en.wikipedia.org/wiki/Universal_approximation_theorem), the number of required neurons increases exponentially with the complexity of the function that is to be approximated. Using multiple, non-linear layers has shown to require fewer units and thus find a more compact representation, thus also avoiding [overfitting](https://en.wikipedia.org/wiki/Overfitting) problems better.\n",
    "    \n",
    "One of the most popular activation functions is the ReLU (Rectified Linear Unit) activation function $f(x) = max(0, x)$\n",
    "because of its simplicity (both in value and derivative).\n",
    "Other popular neural networks are Convolutional Neural Networks (CNN) a subtype of MLPs for image data\n",
    "and Recurrent Neural Networks (e.g. with LSTM or GRU units) that can represent time series.\n",
    "    \n",
    "You can find a neural network playground at https://playground.tensorflow.org/, where you can tinker with a neural network in your browser, testing the influence of different parameters such as different activation functions and number of layers or neurons.\n",
    "    \n",
    "We talk about Deep Learning, when we refer to the usage of neural networks with many many layers and neurons. How deep that is depends on the domain, who you ask, and what time it is.\n",
    "\n",
    "<i>OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.</i> [[ref](https://en.wikipedia.org/wiki/Deep_learning#:~:text=A%20deep%20neural%20network%20(DNN,weights%2C%20biases%2C%20and%20functions.)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network\n",
    "### Motivation\n",
    "From the block above, we have seen, that a neural network transforms features based on weights, biases, and activation functions to form its output. When training a neural network, we adapt the weights and biases, until the network outputs do what we want.\n",
    "To formulate \"what we want\" we need to formulate a loss function.\n",
    "The loss function quantifies the distance between the outputs of the neural network, and the outputs that we would like to see for the given input.\n",
    "* In a **classification problem**, for example, the input could be images of skin, and the discrete output a boolean label of whether that image shows a portion of skin with abnormal growth of skin cells (cancer).\n",
    "* In a **regression problem**, on the other hand, we are interested in continuous outputs such as level of oxygen in your blood.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://miro.medium.com/max/3200/1*ASYpFfDh7XnreU-ygqXonw.png\" alt=\"Regression\" style=\"width: 450px;\"/>\n",
    "    <figcaption><a href=\"https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d\">[ref]</a>\n",
    "</figure> \n",
    "\n",
    "\n",
    "The goal of training a neural network is finding values for our weights and biases that minimize our loss function over a set of data. Training a neural network is thus an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "There are several possible ways of optimizing the weights and biases of a neural network. The most prominent, however, is stochastic gradient descent, a first order method.\n",
    "In gradient descent, we utilize a differentiable loss function $L$ (e.g. a Mean Squared Error) to compute the gradients (partial derivatives of the weights and biases in regard to the loss function). We then use these gradients to update the weights and biases $\\mathbf{w}$ for the next iteration in the direction that minimizes the loss by a learning rate $\\gamma$.\n",
    "$$\\mathbf{w}_{n+1} = \\mathbf{w}_n-\\gamma\\nabla L(\\mathbf{w}_n)$$\n",
    "The first order methods are popular, because higher order methods (using second, third, ... derivatives) are often too computationally inefficient to be used for neural networks with thousands or millions of parameters.\n",
    "Even the first order methods are so expensive to compute, that the neural network hype in the 90s was subdued by the limited hardware that hindered further progress. With the increasing power of GPUs that are well suited for heavily parallelised computation, neural networks gained traction again around 2010.\n",
    "\n",
    "Computing gradients can be done in three ways:\n",
    "- **Manual differentiation** based on rules you learned in school. This is definitely the computationally most efficient way, but is not feasible for neural networks that are many layers deep.\n",
    "- Approximation through **[finite differences](https://en.wikipedia.org/wiki/Finite_difference_method)** as an automated alternative has the downsides of numerical errors based on finite resolution. It is also computationally inefficient.\n",
    "- **Symbolic differentiation** is an *automated* version of manual differentiation, but experiences a problem called expression swell, where the expressions, required to represent the derivative of a function is often longer and more complex than the original expression (e.g. product rule fg -> fg' + f'g). So again, not very applicable for networks with many layers. \n",
    "- **Automatic differentiation** is our best option and builds on the ideas of dynamic programming and computational graphs. For a good explanation of automatic differentiation, please refer to this [blog](https://colah.github.io/posts/2015-08-Backprop/) and this [video](https://www.youtube.com/watch?v=MswxJw-8PvE)\n",
    "\n",
    "In most applications, the <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagation algorithm</a>  (a special case of automatic differentiation) is used to <a href=\"https://colah.github.io/posts/2015-08-Backprop/\">calculate the gradients</a>.\n",
    "Lucky for us, we do not have to implement that ourselves, but can rely on deep learning frameworks such as\n",
    "<a href=\"https://pytorch.org/\">pytorch</a>,\n",
    "<a href=\"https://www.tensorflow.org/\">tensorflow</a>, or\n",
    "<a href=\"https://github.com/google/jax\">JAX</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Simple polynomial\n",
    "The rest of this notebook will give a very short introduction to the deep learning framework [pytorch](https://pytorch.org/).\n",
    "The introduction is mostly copied from [pytorch's excellent documentation](https://pytorch.org/tutorials/).\n",
    "In this example, we will look at a regression problem.\n",
    "We want to use a very small neural network that receives the first three powers of x ($x$, $x^2$, and $x^3$) as its three inputs to approximate a sine function with a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Use the nn package to define our model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# Print out all the parameters of the model for which gradients will be computed\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our model has now 3 weights and one bias that weigh the input to our neuron. Please note that we did not use an activation function in the neural network for now.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src=\"media/mlp.png\" alt=\"MLP\" style=\"width: 400px;\"/>\n",
    "</figure> \n",
    "\n",
    "\n",
    "We will not prepare the inputs to our neural network as well as the ground truth data, the actual values of a sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Print every 200th element of the data\n",
    "print(\"Ground truth data:\\n\", y[0::200], \"\\n\")\n",
    "print(\"Inputs to the neural network:\\n\", xx[0::200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have tensors \n",
    "- x: that holds 2000 elements in equally spaced values from $-\\pi$ to $\\pi$\n",
    "- y: the actual values of $\\sin(x)$\n",
    "- xx: of shape (2000, 3) that holds $x$, $x^2$, and $x^3$ for every value in x\n",
    "\n",
    "To quantify how well our model is doing with the current set of weights, we need a loss function.\n",
    "This one will be the standard Mean Squared Error loss that computes the squared error between our model's predictions, and the actual sine values.\n",
    "$$\n",
    "\\operatorname{MSE}=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2\n",
    "= \\frac{1}{n}\\sum_{i=1}^n(\\sin(x)-\\operatorname{model}(x, x^2, x^3))^2\n",
    "$$\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://cdn-media-1.freecodecamp.org/images/MNskFmGPKuQfMLdmpkT-X7-8w2cJXulP3683\" alt=\"MSE\" style=\"width: 500px;\"/>\n",
    "    <figcaption><i>Example for Mean Squared Error loss for regression of a linear function</i> <a href=\"https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/\">[ref]</a>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the optimizer that will be used to change the weights and bias of our neural network based on computed gradients of the weights and bias in regard to our loss function. \n",
    "We will use the classic [Stochastic Gradient Descenct](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) [optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD).\n",
    "The optimizer also requires a learning rate $\\gamma$ that specifies *by how much* the parameters should be changed in direction of the gradient.\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\begin{split} \n",
    "\\theta_{t+1} = \\theta_{t}-\\gamma\\nabla L(\\theta_{t})\n",
    " = \\theta_{t}-\\gamma g_{t}\n",
    "\\end{split} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "# Pass the model's parameters and the learning rate to the constructor of the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is set up for the actual machine learning part.\n",
    "\n",
    "We will perform the following steps 4000 times (called epochs) to iteratively change the neural network parameters.\n",
    "1. Compute the outputs of the neural network for each data point.\n",
    "2. Compute the loss between predicted outputs and ground truth.\n",
    "3. Clean up the buffer variables inside the optimizer.\n",
    "4. Compute the gradients from the loss function.\n",
    "5. Perform one step of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "# Object for generating a pretty plot of the loss.\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for t in range(4500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Send it to the plot below.\n",
    "    if t % 100 == 99:\n",
    "        logs = {'loss': loss.item()}\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the loss decreases over the epochs (only every 100th epoch is logged, though). The amount of change, however, becomes less noticable in the later stages of training.\n",
    "\n",
    "The learned model now represents a third order polynomial with the following bias and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the output of the polynomial looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, y, \"--\", label=\"sine\")\n",
    "plt.plot(x, model(xx).detach().numpy(), label=\"model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger Model\n",
    "As you can see, there is still a noticable error between the learned model and the actual sine wave.\n",
    "Let's see what happens if we increase the model complexity by adding more layers, neurons, and hyperbolic tangent activation functions.\n",
    "<figure>\n",
    "    <img src=\"https://pytorch.org/docs/stable/_images/Tanh.png\" alt=\"Topology\" style=\"width: 600px;\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model\n",
    "bigger_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 5),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(5, 5),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(5, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# Print out all the parameters of the model for which gradients will be computed\n",
    "for name, param in bigger_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"media/mlp2.png\" alt=\"MLP2\" style=\"width: 600px;\"/>\n",
    "</figure> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the model's parameters and the learning rate to the constructor of a new optimizer\n",
    "new_optimizer = torch.optim.SGD(bigger_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Object for generating a pretty plot of the loss.\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for t in range(int(4e4)):\n",
    "    y_pred = bigger_model(xx)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        logs = {'loss': loss.item()}\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "\n",
    "    new_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    new_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.plot(x, y, \"--\", label=\"sine\")\n",
    "plt.plot(x, model(xx).detach().numpy(), label=\"model\")\n",
    "plt.plot(x, bigger_model(xx).detach().numpy(), label=\"bigger model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... is a bigger model always better?\n",
    "Not at all! The model should always have the smallest size possible to approximate the underlying function to a desired degree of accuracy. Otherwise training will likely fail, or take much longer.\n",
    "Tuning the model size is more art than science, and often involves trying out random combinations, peeking into papers with related problems, and then iteratively increasing/decreasing model sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
